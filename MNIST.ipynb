{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9e6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "841ef1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MNIST dataset has a training size of 60000 examples\n",
      "The MNIST dataset has a test size of 10000 examples\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"The MNIST dataset has a training size of %d examples\" %len(X_train))\n",
    "print(\"The MNIST dataset has a test size of %d examples\" %len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655a0533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shaoe: (60000, 28, 28)\n",
      "60000 train smaples\n",
      "10000 test smaples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "\n",
    "print('X_train shaoe:', X_train.shape)\n",
    "print(X_train.shape[0], 'train smaples')\n",
    "print(X_test.shape[0], 'test smaples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc878de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer-valued labels:\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "One-hot labels:\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "num_classes = 10 \n",
    "# print first ten (integer-valued) training labels\n",
    "print('Integer-valued labels:')\n",
    "print(y_train[:10])\n",
    "\n",
    "# one-hot encode the labels\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# print first ten (one-hot) training labels\n",
    "print('One-hot labels:')\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "950f3ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (28, 28, 1)\n",
      "x_train shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print('input_shape: ', input_shape)\n",
    "print('x_train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aba7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                200768    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 220,234\n",
      "Trainable params: 220,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 1\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# build the model object\n",
    "model = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "# POOL_1: downsample the image to choose the best features \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88f33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce7e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1875/1875 - 13s - loss: 0.1374 - accuracy: 0.9577 - val_loss: 0.0588 - val_accuracy: 0.9814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05878, saving model to model.weights.best.hdf5\n",
      "Epoch 2/12\n",
      "1875/1875 - 9s - loss: 0.0437 - accuracy: 0.9865 - val_loss: 0.0480 - val_accuracy: 0.9843\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05878 to 0.04797, saving model to model.weights.best.hdf5\n",
      "Epoch 3/12\n",
      "1875/1875 - 9s - loss: 0.0316 - accuracy: 0.9905 - val_loss: 0.0307 - val_accuracy: 0.9888\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04797 to 0.03071, saving model to model.weights.best.hdf5\n",
      "Epoch 4/12\n",
      "1875/1875 - 8s - loss: 0.0249 - accuracy: 0.9930 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03071\n",
      "Epoch 5/12\n",
      "1875/1875 - 9s - loss: 0.0203 - accuracy: 0.9942 - val_loss: 0.0258 - val_accuracy: 0.9914\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03071 to 0.02584, saving model to model.weights.best.hdf5\n",
      "Epoch 6/12\n",
      "1875/1875 - 9s - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.0345 - val_accuracy: 0.9912\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02584\n",
      "Epoch 7/12\n",
      "1875/1875 - 9s - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0348 - val_accuracy: 0.9906\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02584\n",
      "Epoch 8/12\n",
      "1875/1875 - 9s - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.0317 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02584\n",
      "Epoch 9/12\n",
      "1875/1875 - 9s - loss: 0.0120 - accuracy: 0.9968 - val_loss: 0.0422 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02584\n",
      "Epoch 10/12\n",
      "1875/1875 - 9s - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.0412 - val_accuracy: 0.9905\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02584\n",
      "Epoch 11/12\n",
      "1875/1875 - 9s - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0445 - val_accuracy: 0.9895\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02584\n",
      "Epoch 12/12\n",
      "1875/1875 - 10s - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.0515 - val_accuracy: 0.9911\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02584\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model.fit(X_train, y_train, batch_size=32, epochs=12,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], \n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e051a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 425,226\n",
      "Trainable params: 425,226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# build the model object\n",
    "model_1 = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model_1.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model_1.add(Conv2D(64, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "# POOL_1: downsample the image to choose the best features \n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model_1.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "model_1.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model_1.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model_1.add(Dense(64, activation='relu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model_1.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6214fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 17s 17ms/step - loss: 0.1325 - accuracy: 0.9589 - val_loss: 0.0330 - val_accuracy: 0.9892\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03303, saving model to model_1.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.0309 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03303 to 0.03094, saving model to model_1.weights.best.hdf5\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.0301 - val_accuracy: 0.9915\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03094 to 0.03008, saving model to model_1.weights.best.hdf5\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0273 - val_accuracy: 0.9917\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03008 to 0.02732, saving model to model_1.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0236 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02732 to 0.02358, saving model to model_1.weights.best.hdf5\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 0.0285 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02358\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.0253 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02358\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0286 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02358\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0293 - val_accuracy: 0.9946\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02358\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 0.0267 - val_accuracy: 0.9943\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02358\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_1.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model_1.fit(X_train, y_train, batch_size=64, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458bf22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 32)        36896     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 517,546\n",
      "Trainable params: 517,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 3\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# build the model object\n",
    "model_3 = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model_3.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model_3.add(Conv2D(64, kernel_size=(3, 3),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model_3.add(Conv2D(128, kernel_size=(3, 3),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "# POOL_1: downsample the image to choose the best features \n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model_3.add(Conv2D(32, (3, 3),padding='same', activation='relu'))\n",
    "model_3.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "model_3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model_3.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model_3.add(Dense(64, activation='relu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model_3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0f1db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 28s 29ms/step - loss: 0.1444 - accuracy: 0.9561 - val_loss: 0.0336 - val_accuracy: 0.9898\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03356, saving model to model_3.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.0379 - accuracy: 0.9884 - val_loss: 0.0294 - val_accuracy: 0.9912\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03356 to 0.02935, saving model to model_3.weights.best.hdf5\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0278 - accuracy: 0.9918 - val_loss: 0.0270 - val_accuracy: 0.9921\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02935 to 0.02702, saving model to model_3.weights.best.hdf5\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.0221 - accuracy: 0.9935 - val_loss: 0.0221 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02702 to 0.02210, saving model to model_3.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.0234 - val_accuracy: 0.9937\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02210\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 28s 29ms/step - loss: 0.0158 - accuracy: 0.9957 - val_loss: 0.0245 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02210\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.0231 - val_accuracy: 0.9925\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02210\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 28s 30ms/step - loss: 0.0115 - accuracy: 0.9968 - val_loss: 0.0246 - val_accuracy: 0.9950\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02210\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 26s 28ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0363 - val_accuracy: 0.9937\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02210\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 28s 30ms/step - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0364 - val_accuracy: 0.9930\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02210\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_3.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model_3.fit(X_train, y_train, batch_size=64, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efce91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 32)        36896     \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 624,042\n",
      "Trainable params: 624,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 4\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# build the model object\n",
    "model_4 = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model_4.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model_4.add(Conv2D(64, kernel_size=(3, 3),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "model_4.add(Conv2D(128, kernel_size=(3, 3),padding='same',activation='relu',input_shape=(28,28,1)))\n",
    "# POOL_1: downsample the image to choose the best features \n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model_4.add(Conv2D(32, (3, 3),padding='same', activation='relu'))\n",
    "model_4.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "model_4.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model_4.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model_4.add(Dense(64, activation='relu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model_4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb2e125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 29s 30ms/step - loss: 0.1437 - accuracy: 0.9556 - val_loss: 0.0718 - val_accuracy: 0.9773\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07183, saving model to model_4.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0388 - accuracy: 0.9888 - val_loss: 0.0292 - val_accuracy: 0.9905\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07183 to 0.02916, saving model to model_4.weights.best.hdf5\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0275 - accuracy: 0.9919 - val_loss: 0.0295 - val_accuracy: 0.9915\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02916\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0216 - accuracy: 0.9936 - val_loss: 0.0216 - val_accuracy: 0.9931\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02916 to 0.02164, saving model to model_4.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0176 - accuracy: 0.9947 - val_loss: 0.0250 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02164\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 0.0302 - val_accuracy: 0.9921\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02164\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 27s 28ms/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0286 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02164\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0334 - val_accuracy: 0.9924\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02164\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.0541 - val_accuracy: 0.9911\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02164\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 27s 29ms/step - loss: 0.0095 - accuracy: 0.9975 - val_loss: 0.0259 - val_accuracy: 0.9930\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02164\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_4.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model_4.fit(X_train, y_train, batch_size=64, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac8ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 397,546\n",
      "Trainable params: 397,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 5\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# build the model object\n",
    "model_5 = Sequential()\n",
    "\n",
    "# CONV_1: add CONV layer with RELU activation and depth = 32 kernels\n",
    "model_5.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "\n",
    "# POOL_1: downsample the image to choose the best features \n",
    "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CONV_2: here we increase the depth to 64\n",
    "model_5.add(Conv2D(32, (3, 3),padding='same', activation='relu'))\n",
    "model_5.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "model_5.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "# POOL_2: more downsampling\n",
    "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten since too many dimensions, we only want a classification output\n",
    "model_5.add(Flatten())\n",
    "\n",
    "# FC_1: fully connected to get all relevant data\n",
    "model_5.add(Dense(64, activation='elu'))\n",
    "\n",
    "# FC_2: output a softmax to squash the matrix into output probabilities for the 10 classes\n",
    "model_5.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ad78e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 13s 13ms/step - loss: 0.1308 - accuracy: 0.9588 - val_loss: 0.0313 - val_accuracy: 0.9891\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03125, saving model to model_5.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0394 - accuracy: 0.9879 - val_loss: 0.0443 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03125\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.0291 - val_accuracy: 0.9895\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03125 to 0.02909, saving model to model_5.weights.best.hdf5\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 0.0234 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02909 to 0.02343, saving model to model_5.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0235 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02343\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0244 - val_accuracy: 0.9920\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02343\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0120 - accuracy: 0.9967 - val_loss: 0.0300 - val_accuracy: 0.9925\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02343\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0363 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02343\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0304 - val_accuracy: 0.9929\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02343\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.0079 - accuracy: 0.9982 - val_loss: 0.0325 - val_accuracy: 0.9935\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02343\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model_5.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model_5.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "hist = model_5.fit(X_train, y_train, batch_size=64, epochs=10,\n",
    "          validation_data=(X_test, y_test), callbacks=[checkpointer], \n",
    "          verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e4c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
